### YamlMime:PythonClass
uid: azure.ai.ml.entities.Spark
name: Spark
fullName: azure.ai.ml.entities.Spark
module: azure.ai.ml.entities
inheritances:
- azure.ai.ml.entities._builders.base_node.BaseNode
- azure.ai.ml.entities._job.spark_job_entry_mixin.SparkJobEntryMixin
summary: 'Base class for spark node, used for spark component version consumption.


  You should not instantiate this class directly. Instead, you should

  create from builder function: spark.


  Class Resource constructor.'
constructor:
  syntax: 'Spark(*, component: Union[str, azure.ai.ml.entities._component.spark_component.SparkComponent],
    identity: Optional[Union[Dict[str, str], azure.ai.ml.entities._credentials.ManagedIdentityConfiguration,
    azure.ai.ml.entities._credentials.AmlTokenConfiguration, azure.ai.ml.entities._credentials.UserIdentityConfiguration]]
    = None, driver_cores: Optional[int] = None, driver_memory: Optional[str] = None,
    executor_cores: Optional[int] = None, executor_memory: Optional[str] = None, executor_instances:
    Optional[int] = None, dynamic_allocation_enabled: Optional[bool] = None, dynamic_allocation_min_executors:
    Optional[int] = None, dynamic_allocation_max_executors: Optional[int] = None,
    conf: Optional[Dict[str, str]] = None, inputs: Optional[Dict[str, Union[azure.ai.ml.entities._job.pipeline._io.base.NodeOutput,
    azure.ai.ml.entities._inputs_outputs.input.Input, str, bool, int, float, enum.Enum]]]
    = None, outputs: Optional[Dict[str, Union[str, azure.ai.ml.entities._inputs_outputs.output.Output]]]
    = None, compute: Optional[str] = None, resources: Optional[Union[Dict, azure.ai.ml.entities._job.spark_resource_configuration.SparkResourceConfiguration]]
    = None, entry: Optional[Union[Dict[str, str], azure.ai.ml.entities._job.spark_job_entry.SparkJobEntry]]
    = None, py_files: Optional[List[str]] = None, jars: Optional[List[str]] = None,
    files: Optional[List[str]] = None, archives: Optional[List[str]] = None, args:
    Optional[str] = None, **kwargs)'
  parameters:
  - name: name
    description: Name of the resource.
    isRequired: true
    types:
    - <xref:str>
  - name: description
    description: Description of the resource, defaults to None
    isRequired: true
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: tags
    description: Tags can be added, removed, and updated., defaults to None
    isRequired: true
    types:
    - <xref:typing.Optional>[<xref:typing.Dict>]
  - name: properties
    description: The asset property dictionary, defaults to None
    isRequired: true
    types:
    - <xref:typing.Optional>[<xref:typing.Dict>]
  - name: print_as_yaml
    description: 'If set to true, then printing out this resource will produce a YAML-formatted
      object.

      False will force a more-compact printing style. By default, the YAML output
      is only used in jupyter

      notebooks. Be aware that some bookkeeping values are shown only in the non-YAML
      output.'
    types:
    - <xref:bool>
attributes:
- uid: azure.ai.ml.entities.Spark.code
  name: code
- uid: azure.ai.ml.entities.Spark.component
  name: component
- uid: azure.ai.ml.entities.Spark.identity
  name: identity
  summary: Identity that spark job will use while running on compute.
- uid: azure.ai.ml.entities.Spark.resources
  name: resources
  summary: Compute Resource configuration for the job.
